{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Verbose\n",
    "# stack trace bei crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data, datasets\n",
    "import torchtext\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from TwitterPipeline import TwitterPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 762\n",
    "# IN_FILE = 'germeval2018.try.txt'\n",
    "IN_FILE = 'germeval2018.training.txt'\n",
    "IN_FILE_TEST = 'germeval2018.test.txt'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define torchtext.Field instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Fields\n",
    "f_text = data.Field(sequential=True, use_vocab=True)\n",
    "f_pos_tag = data.Field(sequential=True, use_vocab=False,\n",
    "                       pad_token=1, unk_token=0)\n",
    "f_lemma = data.Field(sequential=True, use_vocab=True)\n",
    "f_label = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "fields = [('text', f_text), ('pos', f_pos_tag),\n",
    "          ('lemma', f_lemma), ('label', f_label)]\n",
    "\n",
    "# HINT: don't specify a tokenizer here\n",
    "# assign single fields to map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy pipeline\n",
    "# HINT: a simple one - maybe even without setting the model to use - is easier\n",
    "pipe = TwitterPipeline()\n",
    "# pre-process training data\n",
    "full_examples = pipe.process_data(\n",
    "    IN_FILE, fields)[0]\n",
    "full_ds = data.Dataset(full_examples, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the splitting with torchtext\n",
    "trn_ds, val_ds = full_ds.split(\n",
    "    split_ratio=[0.8, 0.2], stratified=True, random_state=random.seed(SEED))\n",
    "test_examples = pipe.process_data(\n",
    "    IN_FILE_TEST, fields)[0]\n",
    "tst_ds = data.Dataset(test_examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len 16\n",
      "val len 4\n",
      "test len 3398\n"
     ]
    }
   ],
   "source": [
    "print(f'train len {len(trn_ds.examples)}')\n",
    "print(f'val len {len(val_ds.examples)}')\n",
    "print(f'test len {len(tst_ds.examples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "# vec = torchtext.vocab.Vectors('embed_tweets_de_100D_fasttext',\n",
    "#                              cache='/Users/michel/Downloads/')\n",
    "\n",
    "# build vocab\n",
    "# validation + test data should by no means influence the model, so build the vocab just on trn\n",
    "#f_text.build_vocab(trn_ds, vectors=vec)\n",
    "f_text.build_vocab(trn_ds, max_size=20000)\n",
    "f_lemma.build_vocab(trn_ds)\n",
    "f_label.build_vocab(trn_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 247\n",
      "lemma vocab size 227\n",
      "label vocab size 2\n"
     ]
    }
   ],
   "source": [
    "print(f'text vocab size {len(f_text.vocab)}')\n",
    "print(f'lemma vocab size {len(f_lemma.vocab)}')\n",
    "print(f'label vocab size {len(f_label.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator for Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training iterators\n",
    "# Aufteilung in Mini-Batches\n",
    "trn_iter, val_iter, tst_iter = data.BucketIterator.splits((trn_ds, val_ds, tst_ds),\n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          device=-1,\n",
    "                                                          sort_key=lambda t: len(\n",
    "                                                              t.text),\n",
    "                                                          sort_within_batch=False,\n",
    "                                                          repeat=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim=100, hidden_dim=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 1 is output dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x type is Tensor[sentence len, batch size]. Internally pytorch does not use 1-hot\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded type is Tensor[sentence len, batch size, emb dim]\n",
    "\n",
    "        output, hidden_state = self.rnn(embedded)\n",
    "        # output type is Tensor[sentence len, batch size, hidden dim]\n",
    "        # hidden_state type is Tensor[1, batch size, hidden dim]\n",
    "\n",
    "        return self.fc(hidden_state.squeeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrik zur Messung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    return accuracy per batch as ratio of correct/all\n",
    "    \"\"\"\n",
    "\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    # convert into float for division\n",
    "    pred_is_correct = (rounded_preds == y).float()\n",
    "    acc = pred_is_correct.sum()/len(pred_is_correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings-Durchlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, metric):  #Â optimierungs-Interface, criterion=loss-Function= Optimnierungskriterium, metric zum beobachten  \n",
    "    epoch_loss = 0\n",
    "    epoch_meter = 0\n",
    "\n",
    "    model.train()   # Regularisier einschalten, um Overfitting zu verhindern\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(batch.text).squeeze(1)   #y_hat = ^y = Prognose\n",
    "        loss = criterion(y_hat, batch.label)\n",
    "        meter = metric(y_hat, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()   # Trainings-Schritt \n",
    "\n",
    "        epoch_loss += loss.item()   # .item --> skalarer = nativer Wert eines Tensors\n",
    "        epoch_meter += meter.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_meter / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluierung (auf Validation-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, metric):\n",
    "    epoch_loss = 0\n",
    "    epoch_meter = 0\n",
    "\n",
    "    model.eval()   # Regularisierer ausschalten, da beste Werte gesucht werden. \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "            y_hat = model(batch.text).squeeze(1)\n",
    "            loss = criterion(y_hat, batch.label)\n",
    "            meter = metric(y_hat, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_meter += meter.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_meter / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMB_SIZE = 100\n",
    "HID_SIZE = 200\n",
    "NUM_LIN = 3\n",
    "NUM_EPOCH = 5\n",
    "\n",
    "# RNN variant SETUP\n",
    "model = SimpleRNN(len(f_text.vocab), EMB_SIZE, HID_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)    # SGD stochastic gradient descent (etwas alt abe gar nicht schlecht)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 00 - TRN_LOSS: 0.668 - TRN_ACC: 62.50% - VAL_LOSS: 0.687 - VAL_ACC: 50.00%\n",
      "EPOCH: 01 - TRN_LOSS: 0.667 - TRN_ACC: 62.50% - VAL_LOSS: 0.686 - VAL_ACC: 50.00%\n",
      "EPOCH: 02 - TRN_LOSS: 0.666 - TRN_ACC: 62.50% - VAL_LOSS: 0.686 - VAL_ACC: 50.00%\n",
      "EPOCH: 03 - TRN_LOSS: 0.665 - TRN_ACC: 62.50% - VAL_LOSS: 0.685 - VAL_ACC: 50.00%\n",
      "EPOCH: 04 - TRN_LOSS: 0.664 - TRN_ACC: 62.50% - VAL_LOSS: 0.684 - VAL_ACC: 50.00%\n",
      "TEST_LOSS: 0.692, TEST_ACC: 50.54%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    train_loss, train_acc = train(\n",
    "        model, trn_iter, optimizer, criterion, binary_accuracy)\n",
    "    valid_loss, valid_acc = evaluate(\n",
    "        model, val_iter, criterion, binary_accuracy)\n",
    "\n",
    "    print(f'EPOCH: {epoch:02} - TRN_LOSS: {train_loss:.3f} - TRN_ACC: {train_acc*100:.2f}% - VAL_LOSS: {valid_loss:.3f} - VAL_ACC: {valid_acc*100:.2f}%')\n",
    "\n",
    "test_loss, test_acc = evaluate(model, tst_iter, criterion, binary_accuracy)\n",
    "print(f'TEST_LOSS: {test_loss:.3f}, TEST_ACC: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
